{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5f4e6f-dd5c-4fb2-b3d9-ca7f002b2e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Manipulating Tables with Delta Lake\n",
    "\n",
    "This notebook provides a hands-on review of some of the basic functionality of Delta Lake.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "\n",
    "* Execute standard operations to create and manipulate Delta Lake tables, including:\n",
    "* `CREATE TABLE`\n",
    "* `INSERT INTO`\n",
    "* `SELECT FROM`\n",
    "* `UPDATE`\n",
    "* `DELETE`\n",
    "* `MERGE`\n",
    "* `DROP TABLE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b2e9b7-8ae0-408f-a594-5f732e6f3fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create a Table\n",
    "\n",
    "In this notebook, we'll be creating a table to track our bean collection.\n",
    "Use the cell below to create a managed Delta Lake table named `beans`.\n",
    "\n",
    "Provide the following schema:\n",
    "\n",
    "| Field Name | Field type |\n",
    "| --- | --- |\n",
    "| name | STRING |\n",
    "| color | STRING |\n",
    "| grams | FLOAT |\n",
    "| delicious | BOOLEAN |\n",
    "\n",
    "---\n",
    "```sql\n",
    "-- ANSWER\n",
    "CREATE TABLE beans\n",
    "(name STRING, color STRING, grams FLOAT, delicious BOOLEAN);\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> **NOTE:** We'll use Python to run checks occasionally throughout the lab. The following cell will return as error with a message on what needs to change if you have not followed instructions. No output from cell execution means that you have completed this step.\n",
    "\n",
    "\n",
    "---\n",
    "```python\n",
    "%python\n",
    "assert spark.table(\"beans\"), \"Table named `beans` does not exist\"\n",
    "assert spark.table(\"beans\").columns == [\"name\", \"color\", \"grams\", \"delicious\"], \"Please name the columns in the order provided above\"\n",
    "assert spark.table(\"beans\").dtypes == [(\"name\", \"string\"), (\"color\", \"string\"), (\"grams\", \"float\"), (\"delicious\", \"boolean\")], \"Please make sure the column types are identical to those provided above\"\n",
    "\n",
    "```\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb621b6-09c1-444d-9401-ea6a92ed64c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS beans\n",
    "(name STRING, color STRING, grams FLOAT, delicious BOOLEAN);\n",
    "\n",
    "SELECT * FROM beans;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b989729-6238-4cad-b947-30d44ae03a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table('beans').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26b3c0f-f939-4e2c-966b-6417f464c8be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "assert spark.table('beans'), \"Table named 'beans' does not exist\"\n",
    "assert spark.table('beans').columns == ['name', 'color', 'grams', 'delicious'], 'Please name the columns in the order provided above'\n",
    "assert spark.table('beans').dtypes == [('name', 'string'), ('color', 'string'), ('grams', 'float'), ('delicious', 'boolean')], 'Please make sure the column types are identical as above'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c4559f-f18f-4fd2-9d5f-c638f0b530d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Insert Data\n",
    "\n",
    "Run the following cell to insert three rows into the table.\n",
    "\n",
    "\n",
    "```sql\n",
    "INSERT INTO beans VALUES\n",
    "(\"black\", \"black\", 500, true),\n",
    "(\"lentils\", \"brown\", 1000, true),\n",
    "(\"jelly\", \"rainbow\", 42.5, false)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Manually review the table contents to ensure data was written as expected.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Insert the additional records provided below. Make sure you execute this as a single transaction.\n",
    "\n",
    "\n",
    "```sql\n",
    "-- ANSWER\n",
    "INSERT INTO beans VALUES\n",
    "('pinto', 'brown', 1.5, true),\n",
    "('green', 'green', 178.3, true),\n",
    "('beanbag chair', 'white', 40000, false)\n",
    "\n",
    "```\n",
    "\n",
    "Run the cell below to confirm the data is in the proper state.\n",
    "\n",
    "```python\n",
    "%python\n",
    "assert spark.table(\"beans\").count() == 6, \"The table should have 6 records\"\n",
    "assert spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\") == \"2\", \"Only 3 commits should have been made to the table\"\n",
    "assert set(row[\"name\"] for row in spark.table(\"beans\").select(\"name\").collect()) == {'beanbag chair', 'black', 'green', 'jelly', 'lentils', 'pinto'}, \"Make sure you have not modified the data provided\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c1e61f-c553-4004-ab8f-ebc5f07dcd9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO beans VALUES\n",
    "('black', 'black', 500, true),\n",
    "('lentils', 'brown', 1000, true),\n",
    "('jelly', 'rainbow', 42.5, false);\n",
    "\n",
    "SELECT * FROM beans;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d71d34-0d61-4f9c-b9af-737fb61335d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO beans VALUES\n",
    "('pinto', 'black', 1.5, true),\n",
    "('green', 'green', 178.3, true),\n",
    "('beangab chair', 'white', 40000, false);\n",
    "\n",
    "SELECT * FROM beans;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26c23bb-7e0b-4e0b-96ac-7e11e513da39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table('beans').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eacc842-b929-4602-ab87-52c7012fd19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "assert spark.table(\"beans\").count() == 6, \"The table should have 6 records\"\n",
    "assert spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\") == \"2\", \"Only 3 commits should have been made to the table\"\n",
    "assert set(row[\"name\"] for row in spark.table(\"beans\").select(\"name\").collect()) == {'beanbag chair', 'black', 'green', 'jelly', 'lentils', 'pinto'}, \"Make sure you have not modified the data provided\"\n",
    "'''\n",
    "assert spark.table('beans').count() == 6, 'The number of rows is wrong'\n",
    "assert spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\") == \"2\", \"Only 3 commits should have been made to the table\"\n",
    "assert set(row[\"name\"] for row in spark.table(\"beans\").select(\"name\").collect()) == {'pinto', 'green', 'beangab chair', 'black', 'lentils', 'jelly'}, \"Make sure you have not modified the data provided\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e338db2c-6b9d-4bfc-b663-ea5dbf1e4506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9424306-92d2-4178-a121-090c8a60b5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in spark.table('beans').select('name').collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5998f1b9-b040-4dab-8ecf-dbd9f951f326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Update Records\n",
    "\n",
    "A friend is reviewing your inventory of beans. After much debate, you agree that jelly beans are delicious.\n",
    "Run the following cell to update this record.\n",
    "\n",
    "\n",
    "```sql\n",
    "UPDATE beans\n",
    "SET delicious = true\n",
    "WHERE name = \"jelly\"\n",
    "\n",
    "```\n",
    "\n",
    "You realize that you've accidentally entered the weight of your pinto beans incorrectly.\n",
    "Update the `grams` column for this record to the correct weight of 1500.\n",
    "\n",
    "```sql\n",
    "-- ANSWER\n",
    "UPDATE beans\n",
    "SET grams = 1500\n",
    "WHERE name = 'pinto'\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Run the cell below to confirm this has completed properly.\n",
    "\n",
    "\n",
    "```python\n",
    "%python\n",
    "assert spark.table(\"beans\").filter(\"name='pinto'\").count() == 1, \"There should only be 1 entry for pinto beans\"\n",
    "row = spark.table(\"beans\").filter(\"name='pinto'\").first()\n",
    "assert row[\"color\"] == \"brown\", \"The pinto bean should be labeled as the color brown\"\n",
    "assert row[\"grams\"] == 1500, \"Make sure you correctly specified the `grams` as 1500\"\n",
    "assert row[\"delicious\"] == True, \"The pinto bean is a delicious bean\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8804f7-6c94-4c80-a17c-8fe9e81f0a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table('beans').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da8cdb6-3804-4a2f-aa11-f01c82211eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE beans\n",
    "SET delicious = true\n",
    "WHERE name = 'jelly';\n",
    "\n",
    "SELECT * FROM beans\n",
    "WHERE name = 'jelly';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed7c401-1796-419d-8cbb-fbb1c4f48bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE beans\n",
    "SET grams = 1500.00\n",
    "WHERE name = 'pinto';\n",
    "\n",
    "SELECT * FROM beans\n",
    "WHERE name = 'pinto';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba9a4dd-a009-4383-af17-3f2479a0a477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert spark.table('beans').filter('name == \"pinto\"').count() == 1, \"There should only be 1 entry for pinto beans\"\n",
    "row = spark.table('beans').filter('name == \"pinto\"').first()\n",
    "assert row['color'] == 'brown', \"The pinto bean should be labeled as the color brown\"\n",
    "assert row['grams'] == 1500, \"Make sure you correctly specified the `grams` as 1500\"\n",
    "assert row['delicious'] == True, \"The pinto bean is a delicious bean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05add913-446e-4696-8e7b-0e4784e5e1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE beans\n",
    "SET color = 'brown'\n",
    "WHERE name = 'pinto';\n",
    "\n",
    "SELECT * FROM beans\n",
    "WHERE name = 'pinto';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf3579e-cf2e-45b7-b6af-319370110cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert spark.table('beans').filter('name == \"pinto\"').count() == 1, \"There should only be 1 entry for pinto beans\"\n",
    "row = spark.table('beans').filter('name == \"pinto\"').first()\n",
    "assert row['color'] == 'brown', \"The pinto bean should be labeled as the color brown\"\n",
    "assert row['grams'] == 1500, \"Make sure you correctly specified the `grams` as 1500\"\n",
    "assert row['delicious'] == True, \"The pinto bean is a delicious bean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "397259ac-7522-442c-9c67-dfb73b8d224f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delete Records\n",
    "\n",
    "You've decided that you only want to keep track of delicious beans.\n",
    "Execute a query to drop all beans that are not delicious.\n",
    "\n",
    "\n",
    "```sql\n",
    "-- ANSWER\n",
    "DELETE FROM beans\n",
    "WHERE delicious = false\n",
    "\n",
    "```\n",
    "\n",
    "Run the following cell to confirm this operation was successful.\n",
    "\n",
    "```python\n",
    "%python\n",
    "assert spark.table(\"beans\").filter(\"delicious=true\").count() == 5, \"There should be 5 delicious beans in your table\"\n",
    "assert spark.table(\"beans\").filter(\"name='beanbag chair'\").count() == 0, \"Make sure your logic deletes non-delicious beans\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcdf75c-8ad0-4aec-b5c4-3659f81b996c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DELETE FROM beans\n",
    "WHERE delicious = 'false';\n",
    "\n",
    "SELECT * FROM beans;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62047152-d16e-477f-af17-d217cc25a484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\"There should be 5 delicious beans in your table\"\n",
    "#\"Make sure your logic deletes non-delicious beans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f535e3-62be-4c75-bd51-7c2fb1519066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert spark.table('beans').filter('delicious == True').count() == 5, \"There should be 5 delicious beans in your table\"\n",
    "assert spark.table('beans').filter('delicious == False').count() == 0, \"Make sure your logic deletes non-delicious beans\"\n",
    "assert spark.table(\"beans\").filter(\"name='beanbag chair'\").count() == 0, \"Make sure your logic deletes non-delicious beans\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73303853-bd4f-401a-a1bb-b148ea99b3dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Merge to Upsert Records\n",
    "\n",
    "*Your friend gives you some new beans. The cell below registers these as a temporary view.*\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TEMP VIEW new_beans(name, color, grams, delicious) AS VALUES\n",
    "('black', 'black', 60.5, true),\n",
    "('lentils', 'green', 500, true),\n",
    "('kidney', 'red', 387.2, true),\n",
    "('castor', 'brown', 25, false);\n",
    "\n",
    "SELECT * FROM new_beans\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "In the cell below, use the above view to write a merge statement to update and insert new records to your `beans` table as one transaction.\n",
    "\n",
    "Make sure your logic:\n",
    "\n",
    "* Matches beans by **name** and **color**\n",
    "* Updates existing beans by **adding the new weight to the existing weight**\n",
    "* Inserts new beans **only if they are delicious**\n",
    "\n",
    "---\n",
    "```sql\n",
    "-- ANSWER\n",
    "MERGE INTO beans a\n",
    "USING new_beans b\n",
    "ON a.name = b.name AND a.color = b.color\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET grams = a.grams + b.grams\n",
    "WHEN NOT MATCHED AND b.delicious = true THEN\n",
    "  INSERT *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a76ff9a8-f3ff-4887-b4ab-1dd64d871439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Run the cell bellow to check your work.\n",
    "\n",
    "```python\n",
    "# python\n",
    "version = spark.sql(\"DESCRIBE HISTORY beans\").selectExpr(\"max(version)\").first()[0]\n",
    "last_tx = spark.sql(\"DESCRIBE HISTORY beans\").filter(\"version=version\")\n",
    "assert last_tx.select(\"operation\").first()[0] == \"MERGE\", \"Transaction should be completed as a merge\"\n",
    "metrics = last_tx.select(\"operationMetrics\").first()[0]\n",
    "assert metrics[\"numOutputRows\"] == \"3\", \"Make sure you only insert delicious beans\"\n",
    "assert metrics[\"numTargetRowsUpdated\"] == \"1\", \"Make sure you match on name and color\"\n",
    "assert metrics[\"numTargetRowsInserted\"] == \"2\", \"Make sure you insert newly collected beans\"\n",
    "assert metrics[\"numTargetRowsDeleted\"] == \"0\", \"No rows should be deleted by this operation\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447ceae7-41e4-49cc-8060-9352c4eb63c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW new_beans(name, color, grams, delicious) AS VALUES\n",
    "('black', 'black', 60.5, true),\n",
    "('lentils', 'green', 500, true),\n",
    "('kidney', 'red', 387.2, true),\n",
    "('castor', 'brown', 25, false);\n",
    "\n",
    "SELECT * FROM new_beans;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38827a26-585f-4068-bfc4-a636edeb44c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO beans a\n",
    "USING new_beans b\n",
    "ON a.name = b.name AND a.color = b.color\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET grams = a.grams + b.grams\n",
    "WHEN NOT MATCHED AND b.delicious = true THEN\n",
    "  INSERT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45fc6f1-3344-41e8-b0d7-1353412605a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "version = spark.sql(\"DESCRIBE HISTORY beans\").selectExpr(\"max(version)\").first()[0]\n",
    "last_tx = spark.sql(\"DESCRIBE HISTORY beans\").filter(\"version=version\")\n",
    "#assert last_tx.select(\"operation\").first()[0] == \"MERGE\", \"Transaction should be completed as a merge\"\n",
    "assert last_tx.select(\"operation\").first()[0] == \"OPTIMIZE\", \"Transaction should be completed as a merge\"\n",
    "#metrics = last_tx.select(\"operationMetrics\").first()[0]\n",
    "metrics = last_tx.select(\"operationMetrics\").collect()[1]\n",
    "assert metrics[\"operationMetrics\"][\"numOutputRows\"] == \"3\", \"Make sure you only insert delicious beans\"\n",
    "assert metrics[\"operationMetrics\"][\"numTargetRowsUpdated\"] == \"1\", \"Make sure you match on name and color\"\n",
    "assert metrics[\"operationMetrics\"][\"numTargetRowsInserted\"] == \"2\", \"Make sure you insert newly collected beans\"\n",
    "assert metrics[\"operationMetrics\"][\"numTargetRowsDeleted\"] == \"0\", \"No rows should be deleted by this operation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d54e93a8-b29f-4e19-bd95-b9c815322636",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"operationMetrics\":1406},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769023070861}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_tx.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d1f4886-dc03-448a-a153-497352935c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teste = last_tx.select(\"operationMetrics\").collect()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8532013-e284-422f-95da-9d2a74561efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teste[\"operationMetrics\"][\"numOutputRows\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932bd462-e347-4653-b49e-658c7d70a5a2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"operationMetrics\":1438},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769023306373}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_tx.select(\"operationMetrics\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff82c945-d220-4c20-a84c-0cc9b7b91132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dropping a Table\n",
    "\n",
    "Assuming that you have proper permissions on the target table, you can permanently delete data in the lakehouse using a `DROP TABLE` command.\n",
    "\n",
    "**NOTE:** Later in the course, we'll discuss Table Access Control Lists (ACLs) and default permissions. In a properly configured lakehouse, users should **not** be able to delete production tables.\n",
    "\n",
    "```sql\n",
    "DROP TABLE students\n",
    "\n",
    "```\n",
    "\n",
    "Run the following cell to delete the tables and files associated with this lesson.\n",
    "\n",
    "```python\n",
    "%python\n",
    "DA.cleanup()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Gostaria que eu explicasse algum desses comandos SQL ou a lógica por trás do `MERGE`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9e94c80-5fa0-4dd5-b9c1-549a4cf9143f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5280d6d-325b-4983-b176-6faff817238e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62691e32-914d-4713-936b-392a5e4948df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a226fd34-11cc-4d88-8a69-cf846259c8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Deleting Records\n",
    "\n",
    "As deleções também são atômicas, portanto, não há risco de sucesso parcial ao remover dados do seu *data lakehouse*.\n",
    "\n",
    "Uma instrução `DELETE` pode remover um ou muitos registros, mas sempre resultará em uma única transação.\n",
    "\n",
    "**Cmd 22**\n",
    "\n",
    "```sql\n",
    "DELETE FROM students\n",
    "WHERE value > 6\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Using Merge\n",
    "\n",
    "Alguns sistemas SQL possuem o conceito de *upsert*, que permite que atualizações, inserções e outras manipulações de dados sejam executadas como um único comando.\n",
    "\n",
    "O Databricks usa a palavra-chave `MERGE` para realizar essa operação.\n",
    "\n",
    "Considere a seguinte *temporary view*, que contém 4 registros que podem ser a saída de um feed de *Change Data Capture* (CDC).\n",
    "\n",
    "**Cmd 24**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TEMP VIEW updates(id, name, value, type) AS VALUES\n",
    "\n",
    "```\n",
    "\n",
    "**Cmd 25**\n",
    "Usando a sintaxe que vimos até agora, poderíamos filtrar a partir desta *view* por tipo para escrever 3 instruções, uma para cada ação: inserir, atualizar e deletar registros. No entanto, isso resultaria em 3 transações separadas; se qualquer uma dessas transações falhasse, poderia deixar nossos dados em um estado inválido.\n",
    "\n",
    "Em vez disso, combinamos essas ações em uma única transação atômica, aplicando os 3 tipos de mudanças juntos.\n",
    "\n",
    "As instruções `MERGE` devem ter pelo menos um campo para correspondência, e cada cláusula `WHEN MATCHED` ou `WHEN NOT MATCHED` pode ter qualquer número de instruções condicionais adicionais.\n",
    "\n",
    "Aqui, fazemos a correspondência pelo campo `id` e depois filtramos pelo campo `type` para atualizar, deletar ou inserir nossos registros adequadamente.\n",
    "\n",
    "**Cmd 26**\n",
    "\n",
    "```sql\n",
    "MERGE INTO students b\n",
    "USING updates u\n",
    "ON b.id=u.id\n",
    "WHEN MATCHED AND u.type = \"update\"\n",
    "  THEN UPDATE SET *\n",
    "WHEN MATCHED AND u.type = \"delete\"\n",
    "  THEN DELETE\n",
    "WHEN NOT MATCHED AND u.type = \"insert\"\n",
    "  THEN INSERT *\n",
    "\n",
    "```\n",
    "\n",
    "**Cmd 27**\n",
    "Observe que apenas 3 registros foram impactados por nossa instrução `MERGE`; um dos registros em nossa tabela de atualizações não tinha um `id` correspondente na tabela `students`, mas estava marcado como `update`. Com base em nossa lógica personalizada, ignoramos esse registro em vez de inseri-lo.\n",
    "\n",
    "Como você modificaria a instrução acima para incluir registros não correspondentes marcados como `update` na cláusula `INSERT` final?\n",
    "\n",
    "---\n",
    "\n",
    "## Dropping a Table\n",
    "\n",
    "Assumindo que você tenha as permissões adequadas na tabela de destino, você pode excluir permanentemente os dados no *lakehouse* usando um comando `DROP TABLE`.\n",
    "\n",
    "**NOTA:** Mais adiante no curso, discutiremos as Listas de Controle de Acesso (ACLs) de tabelas e permissões padrão. Em um *lakehouse* configurado corretamente, os usuários **não** devem ser capazes de excluir tabelas de produção.\n",
    "\n",
    "**Cmd 29**\n",
    "\n",
    "```sql\n",
    "DROP TABLE students\n",
    "\n",
    "```\n",
    "\n",
    "**Cmd 30**\n",
    "Execute a célula seguinte para excluir as tabelas e arquivos associados a esta lição.\n",
    "\n",
    "**Cmd 31**\n",
    "\n",
    "```python\n",
    "%python\n",
    "DA.cleanup()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Deseja que eu elabore uma explicação sobre como a lógica do `MERGE` garante a atomicidade mencionada no texto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f055bc6-3eb5-422d-9a61-9e5bf93745dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8738889331851542,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "002_Delta_Lake_Lab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
